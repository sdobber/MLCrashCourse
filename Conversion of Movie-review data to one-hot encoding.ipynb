{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion of Movie-review data to one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final exercise of Google's [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/) uses the [ACL 2011 IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)) to train a Neural Network on movie review data. At this step, we are not concerned with building an input pipeline or implementing an effective handling and storage of the data.  \n",
    "\n",
    "The following code converts the movie review data we extracted from a ``.tfrecord``-file in the [previous step](https://github.com/sdobber/MLCrashCourse/blob/master/TFrecord%20Extraction.ipynb) to a one-hot encoded matrix and stores it on the disk for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HDF5\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function handles the conversion to a one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_data_columns (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for creating categorial colum from vocabulary list in one hot encoding\n",
    "function create_data_columns(data, informative_terms)\n",
    "   onehotmat=zeros(length(data), length(informative_terms))\n",
    "   \n",
    "    for i=1:length(data)\n",
    "        string=data[i]\n",
    "        for j=1:length(informative_terms)\n",
    "            if occursin(informative_terms[j], string)\n",
    "                onehotmat[i,j]=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return onehotmat\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = h5open(\"train_data.h5\", \"r\") do file\n",
    "   global train_labels=read(file, \"output_labels\")\n",
    "   global train_features=read(file, \"output_features\")\n",
    "end\n",
    "c = h5open(\"test_data.h5\", \"r\") do file\n",
    "   global test_labels=read(file, \"output_labels\")\n",
    "   global test_features=read(file, \"output_features\")\n",
    "end\n",
    "train_labels=train_labels'\n",
    "test_labels=test_labels';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the full vocabulary file, which can be obtained [here](https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/terms.txt). Put it in the same folder as the Jupyter-file and open it using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=Array{String}(undef, 0)\n",
    "open(\"terms.txt\") do file\n",
    "    for ln in eachline(file)\n",
    "        push!(vocabulary, ln)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the test and training features matrices based on the full vocabulary file. This code does not create sparse matrices and takes a long time to run (about 2h on my laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] unsafe_wrap at ./strings/string.jl:71 [inlined]",
      " [2] _searchindex(::String, ::String, ::Int64) at ./strings/search.jl:153",
      " [3] occursin at ./strings/search.jl:456 [inlined]",
      " [4] create_data_columns(::Array{String,1}, ::Array{String,1}) at ./In[6]:8",
      " [5] top-level scope at In[7]:3"
     ]
    }
   ],
   "source": [
    "# This takes a looong time. Only run it once and save the result\n",
    "train_features_full=create_data_columns(train_features, vocabulary)\n",
    "test_features_full=create_data_columns(test_features, vocabulary);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to disk. The data takes about 13GB of memory in uncompressed state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"IMDB_fullmatrix_datacolumns.jld\", \"train_features_full\", train_features_full, \"test_features_full\", test_features_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
